{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9a_t8LC9CknS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Names:\n",
            "Class 0: setosa\n",
            "Class 1: versicolor\n",
            "Class 2: virginica\n",
            "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            ".. _iris_dataset:\n",
            "\n",
            "Iris plants dataset\n",
            "--------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            ":Number of Instances: 150 (50 in each of three classes)\n",
            ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
            ":Attribute Information:\n",
            "    - sepal length in cm\n",
            "    - sepal width in cm\n",
            "    - petal length in cm\n",
            "    - petal width in cm\n",
            "    - class:\n",
            "            - Iris-Setosa\n",
            "            - Iris-Versicolour\n",
            "            - Iris-Virginica\n",
            "\n",
            ":Summary Statistics:\n",
            "\n",
            "============== ==== ==== ======= ===== ====================\n",
            "                Min  Max   Mean    SD   Class Correlation\n",
            "============== ==== ==== ======= ===== ====================\n",
            "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
            "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
            "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
            "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
            "============== ==== ==== ======= ===== ====================\n",
            "\n",
            ":Missing Attribute Values: None\n",
            ":Class Distribution: 33.3% for each of 3 classes.\n",
            ":Creator: R.A. Fisher\n",
            ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            ":Date: July, 1988\n",
            "\n",
            "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
            "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
            "Machine Learning Repository, which has two wrong data points.\n",
            "\n",
            "This is perhaps the best known database to be found in the\n",
            "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
            "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
            "data set contains 3 classes of 50 instances each, where each class refers to a\n",
            "type of iris plant.  One class is linearly separable from the other 2; the\n",
            "latter are NOT linearly separable from each other.\n",
            "\n",
            "|details-start|\n",
            "**References**\n",
            "|details-split|\n",
            "\n",
            "- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
            "  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
            "  Mathematical Statistics\" (John Wiley, NY, 1950).\n",
            "- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
            "  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
            "- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
            "  Structure and Classification Rule for Recognition in Partially Exposed\n",
            "  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
            "  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
            "- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
            "  on Information Theory, May 1972, 431-433.\n",
            "- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
            "  conceptual clustering system finds 3 classes in the data.\n",
            "- Many, many more ...\n",
            "\n",
            "|details-end|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "#Print the class names\n",
        "class_names = iris.target_names\n",
        "print(\"Class Names:\")\n",
        "for i, name in enumerate(class_names):\n",
        "    print(f\"Class {i}: {name}\")\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "print(feature_names)\n",
        "\n",
        "# Get the description of the dataset\n",
        "description = iris.DESCR\n",
        "\n",
        "# Print the description\n",
        "print(description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZxGnH4qCknV"
      },
      "source": [
        "The data set consists of samples from each of three species of Iris (Iris Setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
        "https://en.wikipedia.org/wiki/Iris_flower_data_set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDDYlbFMCknW"
      },
      "outputs": [],
      "source": [
        "# Determine the unique class labels\n",
        "unique_classes = np.unique(y)\n",
        "\n",
        "# Define how many samples to print per class\n",
        "samples_per_class = 2\n",
        "\n",
        "# Print samples from each class\n",
        "for cls in unique_classes:\n",
        "    print(f\"Class {cls} - {iris.target_names[cls]}:\")\n",
        "    class_indices = np.where(y == cls)[0]\n",
        "    for i, idx in enumerate(class_indices[:samples_per_class]):\n",
        "        print(f\"Sample {i + 1}:\")\n",
        "        print(\"Features:\", X[idx])\n",
        "        print(\"Label:\", y[idx])\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSaPR19sCknX"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2MndWDiCknX"
      },
      "outputs": [],
      "source": [
        "# Convert the dataset to a pandas DataFrame for easy visualization\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['Class'] = iris.target_names[y]\n",
        "\n",
        "# Plot pair plots with class colors\n",
        "sns.pairplot(df, hue='Class', markers=['o', 's', '^'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BapV_zPOCknX"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape, y_train.shape  )\n",
        "print(X_test.shape,  y_test.shape )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fVY8fJfCknY"
      },
      "outputs": [],
      "source": [
        "# Count the occurrences of each class label\n",
        "class_counts_train = np.bincount(y_train)\n",
        "\n",
        "\n",
        "# Generate class labels from 0 to 2\n",
        "class_labels = np.arange(3)\n",
        "\n",
        "# Plot the class distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(class_labels, class_counts_train, tick_label=class_labels, align='center')\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Class Distribution in iris Dataset (Train)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Count the occurrences of each class label\n",
        "class_counts_test = np.bincount(y_test)\n",
        "\n",
        "# Generate class labels from 0 to 2\n",
        "class_labels = np.arange(3)\n",
        "\n",
        "# Plot the class distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(class_labels, class_counts_test, tick_label=class_labels, align='center')\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Class Distribution in iris Dataset (Test)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEtL8tGQCknY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(y_train.shape, y_test.shape  )\n",
        "\n",
        "\n",
        "# One-hot encode the target labels\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "print(y_train_cat.shape, y_test_cat.shape  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZO9IJa8CknY"
      },
      "outputs": [],
      "source": [
        "print(y_train[1], y_train_cat[1]  )\n",
        "print(y_train[99], y_train_cat[99]  )\n",
        "print(y_train[101], y_train_cat[101]  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epCztrWJCknZ"
      },
      "outputs": [],
      "source": [
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu', input_shape=(4,))) # since we have four features, input shape is  4\n",
        "model.add(Dense(3, activation='softmax'))# since we have three clases output size is  3\n",
        "\n",
        "# Visualize the model architecture\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1iACW5NCknZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "#from tensorflow.keras.optimizers import SGD\n",
        "# Compile the model\n",
        "\n",
        "\n",
        "learning_rate = 0.001  # Example learning rate\n",
        "# Create an Adam optimizer with the specified learning rate\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Train the model and store the training history\n",
        "history = model.fit(X_train, y_train_cat, epochs=50, batch_size=8, validation_split=0.1)\n",
        "\n",
        "# Plot training loss and accuracy\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test_cat)\n",
        "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTKWgWSmCkna"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A25txo3FCkna"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(X_train.shape,  X_test.shape )\n",
        "print(y_train.shape,  y_test.shape )\n",
        "\n",
        "# Print all unique class labels in training set\n",
        "unique_labels_train = np.unique(y_train)\n",
        "print(\"Class Labels in Training Set:\", unique_labels_train)\n",
        "\n",
        "# Print all unique class labels in test set\n",
        "unique_labels_test = np.unique(y_test)\n",
        "print(\"Class Labels in Test Set:\", unique_labels_test)\n",
        "\n",
        "\n",
        "\n",
        "# Concatenate train and test labels to get the entire dataset\n",
        "y_all = np.concatenate([y_train, y_test])\n",
        "\n",
        "# Count the occurrences of each class label\n",
        "class_counts = np.bincount(y_all)\n",
        "\n",
        "# Generate class labels from 0 to 9\n",
        "class_labels = np.arange(10)\n",
        "\n",
        "# Plot the class distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(class_labels, class_counts, tick_label=class_labels, align='center')\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Class Distribution in MNIST Dataset')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Count the occurrences of each class label\n",
        "class_counts_train = np.bincount(y_train)\n",
        "\n",
        "# Generate class labels from 0 to 9\n",
        "class_labels = np.arange(10)\n",
        "\n",
        "# Plot the class distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(class_labels, class_counts_train, tick_label=class_labels, align='center')\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Class Distribution in MNIST Dataset (Train)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Count the occurrences of each class label\n",
        "class_counts_test = np.bincount(y_test)\n",
        "\n",
        "# Generate class labels from 0 to 9\n",
        "class_labels = np.arange(10)\n",
        "\n",
        "# Plot the class distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(class_labels, class_counts_test, tick_label=class_labels, align='center')\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Class Distribution in MNIST Dataset (Test)')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knEyC9x2Ckna"
      },
      "outputs": [],
      "source": [
        "# Flatten the images into a 1D array\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "print(X_train.shape,  X_test.shape )\n",
        "print(y_train.shape,  y_test.shape )\n",
        "\n",
        "\n",
        "# Normalize the pixel values to be between 0 and 1\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encode the target labels\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60INdzZ7Ckna"
      },
      "outputs": [],
      "source": [
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Visualize the model architecture\n",
        "# Print the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3IilE2ECkna"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "# Compile the model\n",
        "# Define the learning rate\n",
        "learning_rate = 0.01  # Example learning rate\n",
        "\n",
        "# Create an optimizer with the specified learning rate\n",
        "optimizer = SGD(learning_rate=learning_rate,momentum=0.9)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot training loss and accuracy\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7iLVt9TCknb"
      },
      "outputs": [],
      "source": [
        "# Select random examples from the test set\n",
        "num_examples = 10\n",
        "random_indices = np.random.randint(0, len(X_test), num_examples)\n",
        "X_examples = X_test[random_indices]\n",
        "y_true = y_test[random_indices]\n",
        "\n",
        "# Make predictions for the selected examples\n",
        "y_pred = model.predict(X_examples)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Visualize the images and their true/predicted labels\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(num_examples):\n",
        "    plt.subplot(2, num_examples, i + 1)\n",
        "    plt.imshow(X_examples[i].reshape(28, 28), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"T: {np.argmax(y_true[i])}, Pr: {y_pred_labels[i]}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRdOw6ylCknb"
      },
      "outputs": [],
      "source": [
        "# Find the indices of wrong predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "y_true_labels = np.argmax(y_test, axis=1)\n",
        "wrong_indices = np.where(y_pred_labels != y_true_labels)[0]\n",
        "\n",
        "# Visualize a few wrong predictions\n",
        "num_examples = min(5, len(wrong_indices))\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(num_examples):\n",
        "    index = wrong_indices[i]\n",
        "    plt.subplot(1, num_examples, i + 1)\n",
        "    plt.imshow(X_test[index].reshape(28, 28), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"True: {y_true_labels[index]}, Pred: {y_pred_labels[index]}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTDjLI8BCknb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "y_true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true_labels, y_pred_labels)\n",
        "\n",
        "# Normalize the confusion matrix to get probabilities\n",
        "conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap='Blues')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(np.arange(10) + 0.5, labels=[str(i) for i in range(10)])\n",
        "plt.yticks(np.arange(10) + 0.5, labels=[str(i) for i in range(10)])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_norm, annot=True,  fmt='.2f', cmap='Blues')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(np.arange(10) + 0.5, labels=[str(i) for i in range(10)])\n",
        "plt.yticks(np.arange(10) + 0.5, labels=[str(i) for i in range(10)])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
